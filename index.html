<!DOCTYPE HTML>
<html lang="en">
  <head>
    <title>Yue Chen</title>
    
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Yue Chen">
    <meta name="viewport" content="width=900px, initial-scale=0.8, maximum-scale=2.0, user-scalable=yes">

    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>Y</text></svg>" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    <!-- MathJax -->
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>        
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      });
    </script>
  </head>

  <body>
    <table style="width:100%; max-width:900px; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">

            <!-- Introduction & Profile Photo -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr style="padding:0px">

                  <!-- introduction -->
                  <td style="padding: 20px; width:64%; vertical-align:middle">
                    <p class="webname" style="text-align: center;">
                      Yue Chen &nbsp;|&nbsp; é™ˆè¶Š
                    </p>

                    <p>
                      I am currently a graduate student at <a href="https://english.pku.edu.cn/">Peking University</a> with 
                      Agibot Lab advised by Professor <a href="https://zsdonghao.github.io/">Hao Dong</a>. I am also fortunate to have mentorship from <a href="https://warshallrho.github.io/">Ruihai Wu</a>.
                      
                      
                      I've also had great experiences working at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a> and <a href="https://seed.bytedance.com/en/">Seed ByteDance</a>.
                      
                      
                      My research interest is broadly in Robotics, 3D Computer Vision and large language models (LLMs), 
                      with particular interests in generalizable object manipulation.
                    </p>

                    <p style="text-align:center; margin-top: 0px; margin-bottom: 0px;">
                      <a href="mailto:yuechen020614@gmail.com">Email</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=v8ehFSQAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                      <a href="https://github.com/yuechen0614">Github</a>
                    </p>
                  </td>

                  <!-- profile photo -->
                  <td style="padding: 20px; padding-left: 0px; padding-bottom: 0px; width:20%; max-width:40%">
                    <img 
                    style="width:100%; max-width:100%; object-fit: cover; border-radius: 0%; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);" 
                    alt="profile photo" 
                    src="images/yuechen.JPG" 
                    class="hoverZoomLink">
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- News -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 0px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>News</b></h2>
                    <ul class="news-list">
                      <li> [2025/09] ðŸŽ‰ DexGarmentLab has been accepted to NeurIPS 2025 as <strong style="color: red;">Spotlight Presentation</strong>! </li>
                      <li> [2025/08] ðŸŽ‰ ExeCoder has been accepted to EMNLP 2025 as <strong style="color: red;">Oral Presentation</strong>! </li>
                      <li> [2025/06] ðŸŽ‰ Started my internship at ByteDance Seed Robotics Lab</li>
                      <li> [2025/02] ðŸŽ‰ Garment-Pile has been accepted to CVPR 2025 </li>
                      <li> [2025/02] ðŸŽ‰ Started my internship at Microsoft Research Asia </li>
                      <li> [2025/01] ðŸŽ‰ ET-SEED has been accepted to ICLR 2025 </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Interest -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 20px; padding-bottom: 0px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>Research</b></h2>
                    <p style="margin-top: 10px; margin-bottom: 5px;">
                      I'm open to collaborations on related projects, feel free to contact me!
                    </p>
                    <p style="margin-top: 0px; margin-bottom: 10px;">
                      Papers sorted by recency.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Paper List -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>

                <!-- æ–°å¢žï¼šLearning Part-Aware Dense 3D Feature Field For Generalizable Articulated Object Manipulation -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/pa3ff.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);" alt="PA3FF teaser">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Learning Part-Aware Dense 3D Feature Field For Generalizable Articulated Object Manipulation
                    </span>
                    <br>

                    <i>
                      <span class="strong-author">Yue Chen*</span>,
                      <span class="author">Muqing Jiang*</span>,
                      <span class="author">Ruihai Wu</span>,
                      <span class="author">Kaifeng Zheng</span>,
                      <span class="author">Jiaqi Liang</span>,
                      <span class="author">Chenrui Tie</span>,
                      <span class="author">Haoran Lu</span>,
                      <span class="author">Hao Dong</span>
                    </i>
                    <br>
                    <a href="https://pa3ff.github.io/">project page</a>
                    &nbsp;/&nbsp;
                    <a href="">paper</a>
                    &nbsp;/&nbsp;
                    <a href="">code</a>
                    <br>

                    <!-- <div>
                      <span class="accepted-venue">ICLR 2026 (Under Review)</span>
                      <span class="accepted-venue-detail">International Conference on Learning Representations</span>
                    </div> -->

                    <div>
                      <span class="accepted-venue">Under Review</span> <!-- å¯æ ¹æ®å®žé™…æ”¶å½•çŠ¶æ€ä¿®æ”¹ï¼Œå¦‚"NeurIPS 2025" -->
                    </div>
                    
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We propose PA3FF (Part-Aware 3D Feature Field), a 3D-native dense feature representation that encodes functional part awareness directly from point clouds. Combined with PADP (Part-Aware Diffusion Policy), it enhances sample efficiency and generalization in articulated object manipulationâ€”outperforming 2D/3D baselines (CLIP, DINOv2, Grounded-SAM) on 16 simulated (PartInstruct) and 8 real-world tasks, and enabling downstream tasks like 3D shape correspondence and part segmentation.
                    </div>
                  </td>
                </tr>

                <!-- DexGarmentLab -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/dexgarmentlab.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy
                    </span>
                    <br>

                    <i>
                      <span class="author">Yuran Wang</span>*,
                      <span class="author">Ruihai Wu</span>*,
                      <span class="strong-author">Yue Chen</span>*,
                      <span class="author">Jiarui Wang</span>,
                      <span class="author">Jiaqi Liang</span>,
                      <span class="author">Ziyu Zhu</span>,
                      <span class="author">Haoran Geng</span>,
                      <span class="author">Pieter Abbeel</span>,
                      <span class="author">Jitendra Malik</span>,
                      <span class="author">Hao Dong</span>
                    </i>
                    <br>
                    <a href="https://wayrise.github.io/DexGarmentLab/">project page</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/pdf/2505.11032">paper</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/wayrise/DexGarmentLab">code</a>
                    &nbsp;/&nbsp;
                    <a href="https://huggingface.co/datasets/wayrise/DexGarmentLab/tree/main">data</a>
                    <br>

                    <div>
                      <span class="accepted-venue">NeurIPS 2025</span>
                      <span class="accepted-venue-detail">Neural Information Processing Systems</span>
                      <div style="height:3px;"></div>
                      <span class="award-venue">
                        <i class="fa-solid fa-trophy"></i>
                        <a class="award-venue">
                          Spotlight Presentation
                        </a>
                        <i class="fa-solid fa-trophy"></i>
                      </span>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We introduce DexGarmentLab, a realistic sim environment for bimanual dexterous garment manipulation. Based on this environment, we propose a new benchmark, an efficient data collection pipeline, and a novel policy framework that uses category-level visual correspondences for few-shot garment manipulation.
                    </div>
                  </td>
                </tr>
                
                <!-- æ–°å¢žï¼šExeCoder -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/execoder.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);" alt="ExeCoder teaser">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      ExeCoder: Empowering Large Language Models with Executability Representation for Code Translation
                    </span>
                    <br>

                    <i>
                      <span class="author">Minghua He*</span>,
                      <span class="strong-author">Yue Chen*</span>,
                      <span class="author">Fangkai Yang</span>,
                      <span class="author">Pu Zhao</span>,
                      <span class="author">Wenjie Yin</span>,
                      <span class="author">Yu Kang</span>,
                      <span class="author">Qingwei Lin</span>,
                      <span class="author">Saravan Rajmohan</span>,
                      <span class="author">Dongmei Zhang</span>
                    </i>
                    <br>
                    <a href="https://arxiv.org/abs/2501.18460">paper</a>
                    &nbsp;/&nbsp;
                    <a href="">project page</a>
                    &nbsp;/&nbsp;
                    <a href="">code</a>
                    <br>

                    <div>
                      <span class="accepted-venue">EMNLP 2025</span>
                      <span class="accepted-venue-detail">Empirical Methods in Natural Language Processing</span>
                      <div style="height:3px;"></div>
                      <span class="award-venue">
                        <i class="fa-solid fa-trophy"></i>
                        <a class="award-venue">
                          Oral Presentation
                        </a>
                        <i class="fa-solid fa-trophy"></i>
                      </span>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We introduce ExeCoder, a code translation framework that equips large language models (LLMs) with explicit "executability representation." It models code properties like syntax validity, runtime dependency, and type consistency as structured features, fusing them into the translation process to generate more accurate, runnable code across programming languages (e.g., Python â†” Java, C++ â†” Rust).
                    </div>
                  </td>
                </tr>


                <!-- æ–°å¢žï¼šDuet: Joint Exploration of Userâ€“Item Profiles -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/duet.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);" alt="DUET teaser">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Duet: Joint Exploration of Userâ€“Item Profiles
                    </span>
                    <br>

                    <i>
                      <span class="strong-author">Yue Chen</span>,
                      <span class="author">Lu Wang</span>,
                      <span class="author">Minjie Hong</span>,
                      <span class="author">Pu Zhao</span>,
                      <span class="author">Fangkai Yang</span>,
                      <span class="author">Yifei Dong</span>,
                      <span class="author">Minghua He</span>,
                      <span class="author">Nan Hu</span>,
                      <span class="author">Jianjin Zhang</span>,
                      <span class="author">Zhiwei Dai</span>,
                      <span class="author">Yuefeng Zhan</span>,
                      <span class="author">Weihao Han</span>,
                      <span class="author">Hao Sun</span>,
                      <span class="author">Qingwei Lin</span>,
                      <span class="author">Weiwei Deng</span>,
                      <span class="author">Feng Sun</span>,
                      <span class="author">Qi Zhang</span>,
                      <span class="author">Saravan Rajmohan</span>,
                      <span class="author">Dongmei Zhang</span>
                    </i>
                    <br>
                    <a href="https://duetreview.github.io/">project page</a>
                    &nbsp;/&nbsp;
                    <a href="">paper</a>
                    &nbsp;/&nbsp;
                    <a href="">code</a>
                    <br>

                    <!-- <div>
                      <span class="accepted-venue">ICLR 2026 (Under Review)</span> 
                      <span class="accepted-venue-detail">International Conference on Learning Representations</span>
                    </div> -->
                    
                    <div>
                      <span class="accepted-venue">Under Review</span> <!-- å¯æ ¹æ®å®žé™…æ”¶å½•çŠ¶æ€ä¿®æ”¹ï¼Œå¦‚"NeurIPS 2025" -->
                    </div>

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We propose DUET, a closed-loop framework for joint exploration of user-item textual profiles in recommendation systems. It distills raw data into concise cues, expands them into rich profiles via self-prompt construction, and optimizes profiles jointly with reinforcement learning (GRPO) using downstream recommendation feedbackâ€”outperforming baselines on Amazon Music/Book and Yelp datasets, while enabling interpretable LLM-compatible representations.
                    </div>
                  </td>
                </tr>


                <!-- æ–°å¢žï¼šWarriorMath -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/warriormath.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);" alt="WarriorMath teaser">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework
                    </span>
                    <br>

                    <i>
                      <span class="strong-author">Yue Chen*</span>,
                      <span class="author">Minghua He*</span>,
                      <span class="author">Fangkai Yang</span>,
                      <span class="author">Pu Zhao</span>,
                      <span class="author">Lu Wang</span>,
                      <span class="author">Yu Kang</span>,
                      <span class="author">Yifei Dong</span>,
                      <span class="author">Yuefeng Zhan</span>,
                      <span class="author">Hao Sun</span>,
                      <span class="author">Qingwei Lin</span>,
                      <span class="author">Saravan Rajmohan</span>,
                      <span class="author">Dongmei Zhang</span>
                    </i>
                    <br>
                    <a href="https://arxiv.org/abs/2508.01245">paper</a>
                    &nbsp;/&nbsp;
                    <a href="">project page</a>
                    &nbsp;/&nbsp;
                    <a href="">code</a>
                    <br>

                    <div>
                      <span class="accepted-venue">Under Review</span> <!-- å¯æ ¹æ®å®žé™…æ”¶å½•çŠ¶æ€ä¿®æ”¹ï¼Œå¦‚"NeurIPS 2025" -->
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We propose WarriorMath, a defect-aware framework to enhance large language models' (LLMs) mathematical reasoning ability. It first detects potential defects in intermediate calculation steps via a specialized detector, then performs targeted correction and iterative refinementâ€”significantly improving accuracy on complex math tasks such as arithmetic reasoning, algebraic problem-solving, and numerical optimization.
                    </div>
                  </td>
                </tr>



                <!-- TrustRAG -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/trustrag.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      TrustRAG: Enhancing Robustness and Trustworthiness in RAG
                    </span>
                    <br>

                    <i>
                      <span class="author">Huichi Zhou</span>*,
                      <span class="author">Kin-Hei Lee</span>*,
                      <span class="author">Zhonghao Zhan</span>*,
                      <span class="strong-author">Yue Chen</span>,
                      <span class="author">Zhenhao Li</span>,
                      <span class="author">Zhaoyang Wang</span>,
                      <span class="author">Hamed Haddadi</span>,
                      <span class="author">Emine Yilmaz</span>
                    </i>
                    <br>
                    
                    <a href="https://trust-rag.github.io/">project page</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/pdf/2501.00879">paper</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/HuichiZhou/TrustRAG">code</a>
                    <br>

                    <div>
                      <span class="accepted-venue">Under Review</span>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We introduce TrustRAG, a robust Retrieval-Augmented Generation (RAG) framework. It defends against corpus poisoning attacks by a two-stage mechanism: identifying potential attack patterns with K-means clustering and detecting malicious docs via self-assessment.
                    </div>
                  </td>
                </tr>

                <!-- Garment Pile -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/garmentpile.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation
                    </span>
                    <br>

                    <i>
                      <span class="author">Ruihai Wu</span>*,
                      <span class="author">Ziyu Zhu</span>*,
                      <span class="author">Yuran Wang</span>*,
                      <span class="strong-author">Yue Chen</span>,
                      <span class="author">Jiarui Wang</span>,
                      <span class="author">Hao Dong</span>
                    </i>
                    <br>
                    
                    <a href="https://garmentpile.github.io/">project page</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/abs/2503.09243">paper</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/AlwaySleepy/Garment-Pile">code</a>
                    <br>

                    <div>
                      <span class="accepted-venue">CVPR 2025</span>&nbsp;
                      <span class="accepted-venue-detail">Computer Vision and Pattern Recognition</span>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We study the novel task of cluttered garments manipulation using dense visual affordance, with generalization towards diverse states, and propose a novel adaptation module to reorganize cluttered garments into configurations conducive to manipulation.
                    </div>
                  </td>
                </tr>

                <!-- ET-SEED -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/etseed.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy
                    </span>
                    <br>

                    <i>
                      <span class="author">Chenrui Tie</span>*,
                      <span class="strong-author">Yue Chen</span>*,
                      <span class="author">Ruihai Wu</span>*,
                      <span class="author">Boxuan Dong</span>,
                      <span class="author">Zeyi Li</span>,
                      <span class="author">Chongkai Gao</span>,
                      <span class="author">Hao Dong</span>
                    </i>
                    <br>
                    
                    <a href="https://et-seed.github.io/">project page</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/abs/2411.03990">paper</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/Cold114514/ET-SEED">code</a>
                    &nbsp;/&nbsp;
                    <a href="https://et-seed.github.io/static/videos/ET-SEED_sup_video.mp4">video</a>
                    <br>

                    <div>
                      <span class="accepted-venue">ICLR 2025</span>&nbsp;
                      <span class="accepted-venue-detail">International Conference on Learning Representations</span>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner.
                    </div>
                  </td>
                </tr>

                <!-- EqvAfford -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/eqvafford.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning
                    </span>
                    <br>

                    <i>
                      <span class="strong-author">Yue Chen</span>*,
                      <span class="author">Chenrui Tie</span>*,
                      <span class="author">Ruihai Wu</span>*,
                      <span class="author">Hao Dong</span>
                    </i>
                    <br>
                    
                    <a href="https://arxiv.org/pdf/2408.01953">paper</a>
                    <br>

                    <div>
                      <span class="accepted-venue">CVPR 2024 Workshop EquiVision</span>&nbsp;
                      <span class="accepted-venue-detail">Computer Vision and Pattern Recognition</span>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      We propose EqvAfford framework, with novel designs to guarantee the SE(3) equivariance in point-level affordance learning for downstream robotic manipulation.
                    </div>
                  </td>
                </tr>

              </tbody>
            </table>

            <!-- Honors -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-top: 20px; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 0px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>Honors and Awards</b></h2>
                    <ul class="award-list">
                      <li>
                        <span style="font-size: 16px;">Provincial Outstanding Graduates</span>
                        <span style="font-size: 16px;" class="year"><i>2024</i></span>
                      </li>
                      <li>
                        <span style="font-size: 16px;">Sishiyanghua Medal (Only 10 in university)</span>
                        <span style="font-size: 16px;" class="year"><i>2023</i></span>
                      </li>
                      <li>
                        <span style="font-size: 16px;">National Scholarship</span>
                        <span style="font-size: 16px;" class="year"><i>2022 & 2023</i></span>
                      </li>
                      <li>
                        <span style="font-size: 16px;">First Prize, China Undergraduate Mathematical Contest in Modeling</span>
                        <span style="font-size: 16px;" class="year"><i>2022</i></span>
                      </li>
                      <li>
                        <span style="font-size: 16px;">CCPC & ACM-ICPC Regional Silver Medal (Guilin Site & Hangzhou Site)</span>
                        <span style="font-size: 16px;" class="year"><i>2022</i></span>
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Experience -->
            <!-- <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 10px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>Experience</b></h2>
                  </td>
                </tr>
              </tbody>
            </table> -->

            <!-- Example Experience Entry 1 -->
            <!-- <table style="width:98%; margin-left:20px; border:none; padding:0px;">
              <tbody>
                <tr>
                  <td style="padding:25px; width:15%; vertical-align:middle; padding-top:15px;">
                    <img src="images/example_logo.png" style="width:100%; height:auto;">
                  </td>
                  <td style="padding-top:20px; padding-left:20px; padding-right:20px; width:85%; vertical-align:top;">
                    <h3>
                      Example Institution Name, Country
                      <span class="dates">2024.01 - Present</span>
                    </h3>
                    <b>Position Title</b>
                    <br>
                    Research Advisor: Prof. <a href="#">Advisor Name</a>
                  </td>
                </tr>
              </tbody>
            </table> -->

            <!-- Example Experience Entry 2 -->
            <!-- <table style="width:98%; margin-left:20px; border:none; padding:0px;">
              <tbody>
                <tr>
                  <td style="padding:25px; width:15%; vertical-align:middle; padding-top:15px;">
                    <img src="images/example_logo2.png" style="width:100%; height:auto;">
                  </td>
                  <td style="padding-top:20px; padding-left:20px; padding-right:20px; width:85%; vertical-align:top;">
                    <h3>
                      Another Institution Name, Country
                      <span class="dates">2023.06 - 2023.12</span>
                    </h3>
                    <b>Position Title</b>
                    <br>
                    Research Advisor: Prof. <a href="#">Advisor Name</a>
                  </td>
                </tr>
              </tbody>
            </table> -->

            <!-- Academic Service -->
            <!-- <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 10px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>Academic Service</b></h2>
                    <p style="margin-top: 10px;">
                      <strong>Reviewer Services:</strong><br>
                      International Conference on Learning Representations (ICLR), 2025<br>
                      International Conference on Machine Learning (ICML), 2025<br>
                      AAAI Conference on Artificial Intelligence (AAAI), 2024, 2025
                    </p>
                  </td>
                </tr>
              </tbody>
            </table> -->

            <!-- Footnote -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right; font-size:small;">
                      Thank you for visiting! Feel free to contact me if you have any questions.
                      <br>
                      This website is designed based on <a href="https://jonbarron.info/" style="font-size: 12px;">Jon Barron</a>.
                      Last Update: August, 2025
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>